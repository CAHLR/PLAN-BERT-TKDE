{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle, importlib, keras, random, Engine, tqdm, copy, json, time, argparse\n",
    "from keras import backend as K\n",
    "import util.Generator as Generator\n",
    "import model.PLANBERT as Transformer\n",
    "import util.Datahelper as dh\n",
    "\n",
    "sys.argv = ' '\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-test', action=\"store_true\", default=True, required=False)\n",
    "parser.add_argument('-ckptname', dest='ckptname', default=None, required=False)\n",
    "\n",
    "parser.add_argument('-nl', dest='num_layers', default=2, required=False)\n",
    "parser.add_argument('-nhd', dest='num_hidden_dims', default=2**9, required=False)\n",
    "parser.add_argument('-nh', dest='num_heads', default=8, required=False)\n",
    "\n",
    "parser.add_argument('-i', dest='use_item_feat', default=True, required=False)\n",
    "parser.add_argument('-u', dest='use_user_feat', default=True, required=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '-pt_sample_func', dest='pt_sample_func', default='(lambda x:x)', required=False)\n",
    "parser.add_argument(\n",
    "    '-pt_sample_param', dest='pt_sample_param', default='0', required=False)\n",
    "parser.add_argument(\n",
    "    '-pt_history_func', dest='pt_history_func', default='np.random.randint', required=False)\n",
    "parser.add_argument(\n",
    "    '-pt_history_param', dest='pt_history_param', default='25', required=False)\n",
    "\n",
    "parser.add_argument('-nonimprove_limit', dest='nonimprove_limit', default=10, required=False)\n",
    "parser.add_argument('-seed', dest='seed', default=0, required=False, type=int)\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "Engine.set_random_seed(args.seed)\n",
    "\n",
    "basic_config = {\n",
    "    'cuda_num' : Engine.GPU_max_free_memory(),\n",
    "    'course_file' : '../datasets/Taobao_data/Taobao.pkl',\n",
    "    'num_times' : 25,\n",
    "    'num_items' : 10000, \n",
    "    'batch_size' : 32, \n",
    "    'feats' : [5507, 69]\n",
    "}\n",
    "    \n",
    "save_name = 'checkpoint/LSTM'\n",
    "print(basic_config)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(basic_config['cuda_num'])\n",
    "session_config = tf.compat.v1.ConfigProto()\n",
    "session_config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=session_config)\n",
    "\n",
    "with open(basic_config['course_file'], 'rb') as f:\n",
    "    user_dict = pickle.load(f)\n",
    "    print('Total Number of Users : ' + str(len(user_dict)))\n",
    "    \n",
    "all_keys = list(user_dict.keys())\n",
    "all_keys.sort()\n",
    "np.random.shuffle(all_keys)\n",
    "#used_keys, _ = dh.list_partition(all_keys, 0.1, seed=0)\n",
    "\n",
    "train_keys, tv_keys = dh.list_partition(all_keys, 0.7, seed=0)\n",
    "test_keys, valid_keys = dh.list_partition(tv_keys, 0.5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.LSTM as LSTM\n",
    "\n",
    "model_config = {\n",
    "    'name' : 'LSTM',\n",
    "    'use_two_direction' : False,\n",
    "    'num_times' : basic_config['num_times'], \n",
    "    'num_items' : 0,\n",
    "    'base_feats' : [\n",
    "        [True, basic_config['num_items'], 'ItemID'],\n",
    "        [True, 1, 'PredictToken'],\n",
    "        [True, basic_config['num_times'], 'AbsoluteDay'],\n",
    "    ],\n",
    "    # [whether the feature is used, the dimension of the feature, the name of feature]\n",
    "    'feats' : [\n",
    "        [args.use_item_feat, basic_config['feats'][0], 'Seller'],\n",
    "        [args.use_item_feat, basic_config['feats'][1], 'Category']\n",
    "    ],\n",
    "    \n",
    "    'embedding_dim' : args.num_hidden_dims,\n",
    "    'num_layers' : args.num_layers,\n",
    "    \n",
    "    'lstm_dropout' : 0,\n",
    "    'l2_reg_penalty_weight' : 0,\n",
    "    'confidence_penalty_weight' : 0,\n",
    "    'lrate' : 1e-4}\n",
    "\n",
    "model = LSTM.LSTM(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.test:\n",
    "    model.load_weights(save_name + '.h5')\n",
    "else:\n",
    "    train_generator_config = {\n",
    "        'name' : None,\n",
    "        'training' : True, \n",
    "        'sample_func' : args.pt_sample_func,\n",
    "        'sample_param' : args.pt_sample_param,\n",
    "        'history_func' : args.pt_history_func,\n",
    "        'history_param' : args.pt_history_param,\n",
    "        'next_basket' : True, \n",
    "        'batch_size' : basic_config['batch_size'],\n",
    "        'shuffle' : True,\n",
    "        'fixed_seed' : False}\n",
    "\n",
    "    train_generator = Generator.TimeMultihotGenerator(\n",
    "        user_dict, train_keys, basic_config, train_generator_config)\n",
    "    valid_generator = Generator.TimeMultihotGenerator(\n",
    "        user_dict, valid_keys, basic_config, train_generator_config)\n",
    "\n",
    "    Engine.fit(\n",
    "        model=model, \n",
    "        train_generator=train_generator, \n",
    "        valid_generator=valid_generator, \n",
    "        epoch_limit=500, \n",
    "        loss_nonimprove_limit=args.nonimprove_limit,\n",
    "        batch_size=basic_config['batch_size'], \n",
    "        use_cosine_lr=True, \n",
    "        model_save_path=None)\n",
    "    \n",
    "    model.save_weights(save_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator_config = {\n",
    "    'training' : False, \n",
    "    'max_sampling' : 10,\n",
    "    'mask_rate' : None,\n",
    "    'historical' : None,\n",
    "    'batch_size' : 16,\n",
    "    'shuffle' : False,\n",
    "    'fixed_seed' : True}\n",
    "\n",
    "\n",
    "results_mat = {'dist':{}, 'top_n':{}}\n",
    "for h in list(range(4)) + list(range(5, 21, 5)):\n",
    "    results_mat['dist'][h] = {}\n",
    "    results_mat['top_n'][h] = {}\n",
    "    for r in [0]:\n",
    "        test_generator_config['sample_func'] = '(lambda x:x)'\n",
    "        test_generator_config['sample_param'] = str(r)\n",
    "        test_generator_config['history_func'] = '(lambda x:x)'\n",
    "        test_generator_config['history_param'] = str(h)\n",
    "        \n",
    "        test_generator_config['name'] = 'H={1}_R={0}'.format(r, h)\n",
    "        test_generator = Generator.AutoRegressiveMultihotGenerator(\n",
    "            user_dict, test_keys, basic_config, test_generator_config)\n",
    "        \n",
    "        print(test_generator.name + ' ' + str(test_generator.batch_size))\n",
    "        recall, recall_per_sem = Engine.test_auto_regressive(model, test_generator, pred_window=[h, basic_config['num_times']], top_n=False)\n",
    "        results_mat['dist'][h][r] = [recall, recall_per_sem]\n",
    "        recall, recall_per_sem = Engine.test_auto_regressive(model, test_generator, pred_window=[h, basic_config['num_times']], top_n=True)\n",
    "        results_mat['top_n'][h][r] = [recall, recall_per_sem]\n",
    "\n",
    "print(save_name)\n",
    "np.save(save_name + '.npy', np.array(results_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
