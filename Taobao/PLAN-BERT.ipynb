{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle, importlib, keras, random, Engine, tqdm, copy, json, time, argparse\n",
    "from keras import backend as K\n",
    "import util.Generator as Generator\n",
    "import model.PLANBERT as Transformer\n",
    "import util.Datahelper as dh\n",
    "\n",
    "sys.argv = ' '\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-test', action=\"store_true\", default=True)\n",
    "parser.add_argument('-ckptname', dest='ckptname', default=None, required=False)\n",
    "\n",
    "parser.add_argument('-nl', dest='num_layers', default=2, required=False)\n",
    "parser.add_argument('-nhd', dest='num_hidden_dims', default=2**9, required=False)\n",
    "parser.add_argument('-nh', dest='num_heads', default=8, required=False)\n",
    "\n",
    "parser.add_argument('-i', dest='use_item_feat', default=True, required=False)\n",
    "parser.add_argument('-u', dest='use_user_feat', default=True, required=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '-pt_sample_func', dest='pt_sample_func', default='(lambda x:x)', required=False)\n",
    "parser.add_argument(\n",
    "    '-pt_sample_param', dest='pt_sample_param', default='0.8 * tab_row', required=False)\n",
    "parser.add_argument(\n",
    "    '-pt_history_func', dest='pt_history_func', default='(lambda x:x)', required=False)\n",
    "parser.add_argument(\n",
    "    '-pt_history_param', dest='pt_history_param', default='0', required=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '-ft_sample_func', dest='ft_sample_func', default='np.random.randint', required=False)\n",
    "parser.add_argument(\n",
    "    '-ft_sample_param', dest='ft_sample_param', default='tab_row', required=False)\n",
    "parser.add_argument(\n",
    "    '-ft_history_func', dest='ft_history_func', default='np.random.randint', required=False)\n",
    "parser.add_argument(\n",
    "    '-ft_history_param', dest='ft_history_param', default='25', required=False)\n",
    "\n",
    "parser.add_argument('-nonimprove_limit', dest='nonimprove_limit', default=10, required=False)\n",
    "parser.add_argument('-seed', dest='seed', default=0, required=False, type=int)\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "Engine.set_random_seed(args.seed)\n",
    "\n",
    "basic_config = {\n",
    "    'cuda_num' : Engine.GPU_max_free_memory(),\n",
    "    'course_file' : '../datasets/Taobao_data/Taobao.pkl',\n",
    "    'num_times' : 25,\n",
    "    'num_items' : 10000, \n",
    "    'batch_size' : 32, \n",
    "    'feats' : [5507, 69]\n",
    "}\n",
    "    \n",
    "save_name = 'checkpoint/PLANBERT'\n",
    "print(basic_config)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(basic_config['cuda_num'])\n",
    "session_config = tf.compat.v1.ConfigProto()\n",
    "session_config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=session_config)\n",
    "\n",
    "with open(basic_config['course_file'], 'rb') as f:\n",
    "    user_dict = pickle.load(f)\n",
    "    print('Total Number of Users : ' + str(len(user_dict)))\n",
    "    \n",
    "all_keys = list(user_dict.keys())\n",
    "all_keys.sort()\n",
    "np.random.shuffle(all_keys)\n",
    "#used_keys, _ = dh.list_partition(all_keys, 0.1, seed=0)\n",
    "\n",
    "train_keys, tv_keys = dh.list_partition(all_keys, 0.7, seed=0)\n",
    "test_keys, valid_keys = dh.list_partition(tv_keys, 0.5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'name' : 'PLANBERT',\n",
    "    'mask_future' : False,\n",
    "    'num_times' : basic_config['num_times'], \n",
    "    'num_items' : 0,\n",
    "    'base_feats' : [\n",
    "        [True, basic_config['num_items'], 'ItemID'],\n",
    "        [True, 1, 'PredictToken'],\n",
    "        [True, basic_config['num_times'], 'AbsoluteDay'],\n",
    "    ],\n",
    "    # [whether the feature is used, the dimension of the feature, the name of feature]\n",
    "    'feats' : [\n",
    "        [args.use_item_feat, basic_config['feats'][0], 'Seller'],\n",
    "        [args.use_item_feat, basic_config['feats'][1], 'Category']\n",
    "    ],\n",
    "    \n",
    "    'embedding_dim' : args.num_hidden_dims,\n",
    "    'num_layers' : args.num_layers,\n",
    "    'num_heads' : args.num_heads,\n",
    "    \n",
    "    'transformer_dropout' : 0.2,\n",
    "    'embedding_dropout' : 0.2,\n",
    "    \n",
    "    'l2_reg_penalty_weight' : 0,\n",
    "    'confidence_penalty_weight' : 0.1,\n",
    "    'lrate' : 1e-4}\n",
    "\n",
    "model = Transformer.Transformer(model_config)\n",
    "# Pretraining : Course-level Masking\n",
    "\n",
    "if args.test:\n",
    "    model.load_weights(save_name + '.h5')\n",
    "else:\n",
    "    train_generator_config = {\n",
    "        'name' : None,\n",
    "        'training' : True, \n",
    "        'sample_func' : args.pt_sample_func,\n",
    "        'sample_param' : args.pt_sample_param,\n",
    "        'history_func' : args.pt_history_func,\n",
    "        'history_param' : args.pt_history_param,\n",
    "\n",
    "        'batch_size' : basic_config['batch_size'],\n",
    "        'shuffle' : True,\n",
    "        'fixed_seed' : False}\n",
    "\n",
    "    train_generator = Generator.TimeMultihotGenerator(\n",
    "        user_dict, train_keys, basic_config, train_generator_config)\n",
    "    valid_generator = Generator.TimeMultihotGenerator(\n",
    "        user_dict, valid_keys, basic_config, train_generator_config)\n",
    "\n",
    "    Engine.fit(\n",
    "        model=model, \n",
    "        train_generator=train_generator, \n",
    "        valid_generator=valid_generator, \n",
    "        epoch_limit=500, \n",
    "        loss_nonimprove_limit=args.nonimprove_limit,\n",
    "        batch_size=basic_config['batch_size'], \n",
    "        use_cosine_lr=True, \n",
    "        model_save_path=None)\n",
    "    \n",
    "    # Fine-tune\n",
    "    train_generator_config = {\n",
    "        'name' : None,\n",
    "        'training' : True, \n",
    "        'sample_func' : args.ft_sample_func,\n",
    "        'sample_param' : args.ft_sample_param,\n",
    "        'history_func' : args.ft_history_func,\n",
    "        'history_param' : args.ft_history_param,\n",
    "\n",
    "        'batch_size' : basic_config['batch_size'],\n",
    "        'shuffle' : True,\n",
    "        'fixed_seed' : False}\n",
    "\n",
    "    train_generator = Generator.TimeMultihotGenerator(\n",
    "        user_dict, train_keys, basic_config, train_generator_config)\n",
    "    valid_generator = Generator.TimeMultihotGenerator(\n",
    "        user_dict, valid_keys, basic_config, train_generator_config)\n",
    "\n",
    "    Engine.fit(\n",
    "        model=model, \n",
    "        train_generator=train_generator, \n",
    "        valid_generator=valid_generator, \n",
    "        epoch_limit=500, \n",
    "        loss_nonimprove_limit=args.nonimprove_limit,\n",
    "        batch_size=basic_config['batch_size'], \n",
    "        use_cosine_lr=True, \n",
    "        model_save_path=None)\n",
    "    \n",
    "    model.save_weights(save_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator_config = {\n",
    "    'training' : False, \n",
    "    'max_sampling' : 25,\n",
    "    'mask_rate' : None,\n",
    "    'historical' : None,\n",
    "    'batch_size' : 16,\n",
    "    'shuffle' : False,\n",
    "    'fixed_seed' : True}\n",
    "\n",
    "results_mat = {}\n",
    "wishlist_mat = {}\n",
    "for h in list(range(4)) + list(range(5, 21, 5)):\n",
    "    results_mat[h] = {}\n",
    "    wishlist_mat[h] = {}\n",
    "    for r in [0,1,2,3,5,7,10,15,20,25]:\n",
    "        test_generator_config['sample_func'] = '(lambda x:x)'\n",
    "        test_generator_config['sample_param'] = str(r)\n",
    "        test_generator_config['history_func'] = '(lambda x:x)'\n",
    "        test_generator_config['history_param'] = str(h)\n",
    "        \n",
    "        test_generator_config['name'] = 'H={1}_R={0}'.format(r, h)\n",
    "        test_generator = Generator.TimeMultihotGenerator(\n",
    "            user_dict, test_keys, basic_config, test_generator_config)\n",
    "        \n",
    "        print(test_generator.name + ' ' + str(test_generator.batch_size))\n",
    "        recall, recall_per_sem = Engine.test(model, test_generator, pred_window=[h, basic_config['num_times']])\n",
    "        results_mat[h][r] = [recall, recall_per_sem]\n",
    "        recall, recall_per_sem = Engine.wishlist_test_onehot(model, test_generator, pred_window=[h, basic_config['num_times']])\n",
    "        wishlist_mat[h][r] = [recall, recall_per_sem]\n",
    "\n",
    "np.save(save_name + '.npy', results_mat)\n",
    "np.save(save_name + '-wishlist.npy', wishlist_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.Model(inputs=model.input, outputs=model.get_layer('transformer_1_self_attention').get_output_at(0)[1])\n",
    "\n",
    "test_generator_config = {\n",
    "    'training' : False, \n",
    "    'max_sampling' : 25,\n",
    "    'mask_rate' : None,\n",
    "    'historical' : None,\n",
    "    'batch_size' : 16,\n",
    "    'shuffle' : False,\n",
    "    'fixed_seed' : True}\n",
    "\n",
    "length = [user_dict[iter].shape[0] for iter in train_keys]\n",
    "max_id = np.argmax(length)\n",
    "\n",
    "output = []\n",
    "input_list = []\n",
    "for h in range(25):\n",
    "    # Pretraining : Course-level Masking\n",
    "    test_generator_config['sample_func'] = '(lambda x:x)'\n",
    "    test_generator_config['sample_param'] = str(5)\n",
    "    test_generator_config['history_func'] = '(lambda x:x)'\n",
    "    test_generator_config['history_param'] = str(h)\n",
    "    test_generator_config['name'] = '4Y R={0}_H={1}'.format(test_generator_config['sample_param'], test_generator_config['history_param'])\n",
    "    test_generator = Generator.TimeMultihotGenerator(\n",
    "        user_dict, train_keys, basic_config, test_generator_config)\n",
    "\n",
    "    input = test_generator.data_generation(train_keys[max_id])\n",
    "    input = [iter[np.newaxis] for iter in input]\n",
    "    \n",
    "    input_list.append(input[0])\n",
    "    \n",
    "    output.append(model1.predict(input))\n",
    "output = np.concatenate(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights = output.sum(1).T[::-1]\n",
    "#weights = weights / np.sum(weights, 0)[np.newaxis,:]\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(weights)\n",
    "ref_list = input_list[0].sum(-1)[0]\n",
    "his_list = input_list[-1].sum(-1)[0]\n",
    "#sem = ['$F_1$','$S_1$','$M_1$','$F_2$','$S_2$','$M_2$','$F_3$','$S_3$','$M_3$','$F_4$','$S_4$','$M_4$',]\n",
    "#sem_y = copy.deepcopy(sem)\n",
    "\n",
    "y_pos = []\n",
    "y_label = []\n",
    "for iter in range(25):\n",
    "    #if int(ref_list[iter]) > 0:\n",
    "    if True:\n",
    "        y_pos.append(iter)\n",
    "        y_label.append('Day ' + str(24-iter) + ':r='+str(int(ref_list[24-iter])) + '/' + str(int(his_list[24-iter])))\n",
    "    \n",
    "ax.xaxis.set_ticks(np.arange(0, 25, 2))\n",
    "ax.xaxis.set_ticklabels(np.arange(0, 25, 2), fontsize=30)\n",
    "#ax.set_xlabel('Grade of Student', fontsize=30)\n",
    "\n",
    "ax.yaxis.set_ticks(y_pos)\n",
    "ax.yaxis.set_ticklabels(y_label, fontsize=30)\n",
    "ax.set_xlabel('Historical Time Slots', fontsize=40)\n",
    "ax.set_ylabel('Importance of Input Vectors', fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
